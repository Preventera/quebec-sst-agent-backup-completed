// Client LLM Gateway pour AgenticSST QuÃ©bec
// src/lib/llmClient.ts - VERSION CORRIGÃ‰E

interface LLMMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

interface LLMRequest {
  provider: 'openai' | 'anthropic';
  model: string;
  messages: LLMMessage[];
  temperature?: number;
  max_tokens?: number;
  org_id?: string;
  user_id?: string;
  agent_name?: string;
}

interface LLMResponse {
  id: string;
  content: string;
  usage?: {
    input_tokens: number;
    output_tokens: number;
    total_tokens: number;
  };
  error?: string;
}

class LLMGatewayClient {
  private gatewayUrl: string;
  private orgId: string;
  private supabaseKey: string;

  constructor(orgId: string = 'preventera') {
    // ðŸ”§ CORRECTION : Utilisation des variables d'environnement
    this.gatewayUrl = import.meta.env.VITE_LLM_GATEWAY_URL || 'https://lljuzduaryqbzrenkfoo.supabase.co/functions/v1/bright-handler';
    this.orgId = import.meta.env.VITE_DEFAULT_ORG_ID || orgId;
    this.supabaseKey = import.meta.env.VITE_SUPABASE_ANON_KEY || '';

    // VÃ©rification des variables critiques
    if (!this.supabaseKey) {
      console.warn('VITE_SUPABASE_ANON_KEY manquant dans .env.local');
    }
  }

  async chat(
    messages: LLMMessage[],
    options: {
      provider?: 'openai' | 'anthropic';
      model?: string;
      temperature?: number;
      max_tokens?: number;
      agent_name?: string;
      user_id?: string;
    } = {}
  ): Promise<LLMResponse> {
    const request: LLMRequest = {
      provider: options.provider || import.meta.env.VITE_DEFAULT_PROVIDER || 'anthropic',
      model: options.model || import.meta.env.VITE_DEFAULT_MODEL || 'claude-3-haiku-20240307',
      messages,
      temperature: options.temperature || 0.7,
      max_tokens: options.max_tokens || 1000,
      org_id: this.orgId,
      user_id: options.user_id,
      agent_name: options.agent_name
    };

    try {
      const response = await fetch(this.gatewayUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          // ðŸ”§ CORRECTION : Ajout de l'authentification
          'Authorization': `Bearer ${this.supabaseKey}`,
          // Headers additionnels pour Supabase Edge Functions
          'apikey': this.supabaseKey
        },
        body: JSON.stringify(request)
      });

      if (!response.ok) {
        // ðŸ”§ CORRECTION : Gestion d'erreurs amÃ©liorÃ©e
        const errorText = await response.text().catch(() => 'Unknown error');
        
        if (response.status === 401) {
          throw new Error('Authentication failed. Check VITE_SUPABASE_ANON_KEY in .env.local');
        }
        if (response.status === 429) {
          throw new Error('Rate limit exceeded. Please try again later.');
        }
        if (response.status === 403) {
          throw new Error('Request blocked for security reasons.');
        }
        
        console.error('Gateway error details:', {
          status: response.status,
          statusText: response.statusText,
          body: errorText
        });
        
        throw new Error(`Gateway error: ${response.status} - ${errorText}`);
      }

      const data = await response.json();
      
      // ðŸ”§ CORRECTION : Gestion robuste de la rÃ©ponse
      if (data.error) {
        throw new Error(`LLM Provider error: ${data.error}`);
      }

      // Normaliser la rÃ©ponse selon le provider
      if (request.provider === 'anthropic') {
        return {
          id: data.id || 'unknown',
          content: data.content?.[0]?.text || data.content || '',
          usage: {
            input_tokens: data.usage?.input_tokens || 0,
            output_tokens: data.usage?.output_tokens || 0,
            total_tokens: (data.usage?.input_tokens || 0) + (data.usage?.output_tokens || 0)
          }
        };
      } else {
        // OpenAI format
        return {
          id: data.id || 'unknown',
          content: data.choices?.[0]?.message?.content || '',
          usage: {
            input_tokens: data.usage?.prompt_tokens || 0,
            output_tokens: data.usage?.completion_tokens || 0,
            total_tokens: data.usage?.total_tokens || 0
          }
        };
      }
    } catch (error) {
      console.error('LLM Gateway error:', error);
      throw error;
    }
  }

  // MÃ©thode spÃ©cialisÃ©e pour les agents SST
  async agentChat(
    agentName: string,
    userMessage: string,
    systemPrompt?: string,
    options: {
      provider?: 'openai' | 'anthropic';
      model?: string;
      userId?: string;
    } = {}
  ): Promise<LLMResponse> {
    const messages: LLMMessage[] = [];
    
    if (systemPrompt) {
      messages.push({ role: 'system', content: systemPrompt });
    }
    
    messages.push({ role: 'user', content: userMessage });

    return this.chat(messages, {
      ...options,
      agent_name: agentName,
      user_id: options.userId
    });
  }

  // MÃ©thode pour conversations multi-tours
  async continueConversation(
    agentName: string,
    conversationHistory: LLMMessage[],
    newMessage: string,
    options: {
      provider?: 'openai' | 'anthropic';
      model?: string;
      userId?: string;
    } = {}
  ): Promise<LLMResponse> {
    const messages = [
      ...conversationHistory,
      { role: 'user' as const, content: newMessage }
    ];

    return this.chat(messages, {
      ...options,
      agent_name: agentName,
      user_id: options.userId
    });
  }

  // ðŸ”§ NOUVEAU : MÃ©thode de test de connexion
  async testConnection(): Promise<boolean> {
    try {
      const response = await this.agentChat(
        'test',
        'Hello test',
        'Respond with just "OK"'
      );
      return response.content.includes('OK');
    } catch (error) {
      console.error('Connection test failed:', error);
      return false;
    }
  }
}

// Instance par dÃ©faut
export const llmClient = new LLMGatewayClient();

// Fonction helper pour crÃ©er des clients par organisation
export function createLLMClient(orgId: string) {
  return new LLMGatewayClient(orgId);
}

// Hooks React pour faciliter l'usage
export function useLLMClient(orgId?: string) {
  return orgId ? createLLMClient(orgId) : llmClient;
}

// ðŸ”§ CORRECTION : Utilitaire pour migration progressive amÃ©liorÃ©
export async function migrateLLMCall(
  legacyCall: () => Promise<string>,
  agentName: string,
  userMessage: string,
  systemPrompt?: string
): Promise<string> {
  try {
    // Essayer d'abord le gateway
    const response = await llmClient.agentChat(agentName, userMessage, systemPrompt);
    return response.content;
  } catch (error) {
    console.warn('Gateway failed, falling back to legacy call:', error);
    // Fallback vers l'ancien appel en cas d'erreur
    return await legacyCall();
  }
}

// ðŸ”§ NOUVEAU : Fonction utilitaire de diagnostic
export async function diagnoseGateway(): Promise<{
  configured: boolean;
  connected: boolean;
  errors: string[];
}> {
  const errors: string[] = [];
  
  // VÃ©rifier la configuration
  const configured = !!(import.meta.env.VITE_SUPABASE_ANON_KEY && import.meta.env.VITE_LLM_GATEWAY_URL);
  if (!configured) {
    if (!import.meta.env.VITE_SUPABASE_ANON_KEY) errors.push('VITE_SUPABASE_ANON_KEY manquant');
    if (!import.meta.env.VITE_LLM_GATEWAY_URL) errors.push('VITE_LLM_GATEWAY_URL manquant');
  }
  
  // Tester la connexion
  let connected = false;
  if (configured) {
    try {
      connected = await llmClient.testConnection();
    } catch (error) {
      errors.push(`Connection failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }
  
  return { configured, connected, errors };
}